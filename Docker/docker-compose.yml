version: "3.9"
services:
  llama:
    image: ghcr.io/ggerganov/llama.cpp:server  # choose a tag that matches your platform
    command: [
      "-m", "/models/YourModel.gguf",
      "-c", "4096",
      "--host", "0.0.0.0",
      "--port", "8000"
    ]
    volumes:
      - ./data/models:/models
    ports:
      - "8000:8000"

  app:
    build: .
    environment:
      - OPENAI_BASE_URL=http://llama:8000/v1
      - OPENAI_API_KEY=sk-noauth
    depends_on:
      - llama
    command: ["scalable_sys", "--prompt", "Say hi via server!"]
